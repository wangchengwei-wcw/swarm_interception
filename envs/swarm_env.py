from __future__ import annotations

import gymnasium as gym
from loguru import logger
import math
import time
import torch
from collections.abc import Sequence

import isaaclab.sim as sim_utils
from isaaclab.assets import Articulation, ArticulationCfg
from isaaclab.envs import DirectMARLEnv, DirectMARLEnvCfg, ViewerCfg
from isaaclab.envs.ui import BaseEnvWindow
from isaaclab.scene import InteractiveSceneCfg
from isaaclab.sim import SimulationCfg
from isaaclab.terrains import TerrainImporterCfg
from isaaclab.utils import configclass
from isaaclab.utils.math import quat_rotate

from envs.quadcopter import CRAZYFLIE_CFG, DJI_FPV_CFG  # isort: skip
from utils.utils import quat_to_ang_between_z_body_and_z_world
from utils.minco import MinJerkOpt
from utils.controller import Controller


class QuadcopterEnvWindow(BaseEnvWindow):
    """Window manager for the Quadcopter environment"""

    def __init__(self, env: QuadcopterSwarmEnv, window_name: str = "IsaacLab"):
        """Initialize the window

        Args:
            env: The environment object
            window_name: The name of the window. Defaults to "IsaacLab"
        """
        # Initialize base window
        super().__init__(env, window_name)
        # Add custom UI elements
        with self.ui_window_elements["main_vstack"]:
            with self.ui_window_elements["debug_frame"]:
                with self.ui_window_elements["debug_vstack"]:
                    # Add command manager visualization
                    self._create_debug_vis_ui_element("targets", self.env)


@configclass
class QuadcopterSwarmEnvCfg(DirectMARLEnvCfg):
    # Change viewer settings
    viewer = ViewerCfg(eye=(-5.0, -5.0, 4.0))

    # Env
    episode_length_s = 60.0
    physics_freq = 200
    control_freq = 100
    mpc_freq = 10
    gui_render_freq = 50
    control_decimation = physics_freq // control_freq
    num_drones = 9  # Number of drones per environment
    decimation = math.ceil(physics_freq / mpc_freq)  # Environment (replan) decimation
    render_decimation = physics_freq // gui_render_freq
    possible_agents = [f"drone_{i}" for i in range(num_drones)]
    observation_spaces = {agent: 13 for agent in possible_agents}
    state_space = 0
    debug_vis = False

    # MINCO trajectory
    num_pieces = 6
    duration = 0.3
    a_max = {agent: 10.0 for agent in possible_agents}
    v_max = {agent: 5.0 for agent in possible_agents}

    # FIXME: @configclass doesn't support the following syntax #^#
    # p_max = {agent: num_pieces * v_max[agent] * duration for agent in possible_agents}
    # action_space = {agent: 3 * (num_pieces + 2) for agent in possible_agents}  # inner_pts 3 x (num_pieces - 1) + tail_pva 3 x 3
    p_max = {agent: 6 * 5.0 * 0.3 for agent in possible_agents}
    action_spaces = {agent: 3 * (6 + 2) for agent in possible_agents}  # inner_pts 3 x (num_pieces - 1) + tail_pva 3 x 3

    clip_action = 100  # Default bound for box action spaces in IsaacLab Sb3VecEnvWrapper

    ui_window_class_type = QuadcopterEnvWindow

    # Simulation
    sim: SimulationCfg = SimulationCfg(
        dt=1 / 200,
        render_interval=render_decimation,
        physics_material=sim_utils.RigidBodyMaterialCfg(
            friction_combine_mode="multiply",
            restitution_combine_mode="multiply",
            static_friction=1.0,
            dynamic_friction=1.0,
            restitution=0.0,
        ),
    )
    terrain = TerrainImporterCfg(
        prim_path="/World/ground",
        terrain_type="plane",
        collision_group=-1,
        physics_material=sim_utils.RigidBodyMaterialCfg(
            friction_combine_mode="multiply",
            restitution_combine_mode="multiply",
            static_friction=1.0,
            dynamic_friction=1.0,
            restitution=0.0,
        ),
        debug_vis=False,
    )

    # Scene
    scene: InteractiveSceneCfg = InteractiveSceneCfg(num_envs=4096, env_spacing=6, replicate_physics=True)

    # Robot
    drone_cfg: ArticulationCfg = DJI_FPV_CFG.copy()
    init_gap = 0.8


class QuadcopterSwarmEnv(DirectMARLEnv):
    cfg: QuadcopterSwarmEnvCfg

    def __init__(self, cfg: QuadcopterSwarmEnvCfg, render_mode: str | None = None, **kwargs):
        super().__init__(cfg, render_mode, **kwargs)

        if self.cfg.decimation < 1 or self.cfg.control_decimation < 1:
            raise ValueError("Replan and control decimation must be greater than or equal to 1 #^#")

        if 1 / self.cfg.mpc_freq > self.cfg.num_pieces * self.cfg.duration:
            raise ValueError("Replan period must be less than or equal to the total trajectory duration #^#")

        # Get specific body indices for each drone
        self.body_ids = {agent: self.robots[agent].find_bodies("body")[0] for agent in self.cfg.possible_agents}

        self.robot_masses = {agent: self.robots[agent].root_physx_view.get_masses()[0, 0] for agent in self.cfg.possible_agents}
        self.robot_inertias = {agent: self.robots[agent].root_physx_view.get_inertias()[0, 0] for agent in self.cfg.possible_agents}
        self.gravity = torch.tensor(self.sim.cfg.gravity, device=self.device)

        # Traj
        self.waypoints, self.trajs = {}, {}
        self.has_prev_traj = torch.tensor([False] * self.num_envs, device=self.device)

        # Controllers
        self.actions = {}
        self.a_desired_total, self.thrust_desired, self._thrust_desired, self.q_desired, self.w_desired, self.m_desired = {}, {}, {}, {}, {}, {}
        self.controllers = {
            agent: Controller(self.step_dt, self.gravity, self.robot_masses[agent].to(self.device), self.robot_inertias[agent].to(self.device))
            for agent in self.cfg.possible_agents
        }
        self.control_counter = 0

    def _setup_scene(self):
        self.robots = {}
        points_per_side = math.ceil(math.sqrt(self.cfg.num_drones))
        side_length = (points_per_side - 1) * self.cfg.init_gap
        for i, agent in enumerate(self.cfg.possible_agents):
            row = i // points_per_side
            col = i % points_per_side
            init_pos = (col * self.cfg.init_gap - side_length / 2, row * self.cfg.init_gap - side_length / 2, 1.0)

            drone = Articulation(
                self.cfg.drone_cfg.replace(
                    prim_path=f"/World/envs/env_.*/Robot_{i}",
                    init_state=self.cfg.drone_cfg.init_state.replace(pos=init_pos),
                )
            )
            self.robots[agent] = drone
            self.scene.articulations[agent] = drone

        self.cfg.terrain.num_envs = self.scene.cfg.num_envs
        self.cfg.terrain.env_spacing = self.scene.cfg.env_spacing
        self.terrain = self.cfg.terrain.class_type(self.cfg.terrain)

        # Clone, filter, and replicate
        self.scene.clone_environments(copy_from_source=False)

        # Add lights
        light_cfg = sim_utils.DomeLightCfg(intensity=2000.0, color=(0.75, 0.75, 0.75))
        light_cfg.func("/World/Light", light_cfg)

    def _pre_physics_step(self, actions: dict[str, torch.Tensor]) -> None:
        head_pva_all, inner_pts_all, tail_pva_all, durations_all = [], [], [], []
        for agent in self.possible_agents:
            # Action parametrization: waypoints in body frame
            self.waypoints[agent] = actions[agent].clone().clamp(-self.cfg.clip_action, self.cfg.clip_action) / self.cfg.clip_action

            # Head states
            p_odom = self.robots[agent].data.root_state_w[:, :3]
            q_odom = self.robots[agent].data.root_state_w[:, 3:7]
            v_odom = self.robots[agent].data.root_state_w[:, 7:10]
            a_odom = torch.zeros_like(v_odom)
            if self.trajs:
                a_odom = torch.where(self.has_prev_traj.unsqueeze(1), self.trajs[agent].get_acc(self.execution_time), a_odom)
            head_pva = torch.stack([p_odom, v_odom, a_odom], dim=2)
            head_pva_all.append(head_pva)

            # Waypoints
            inner_pts = torch.zeros((self.num_envs, 3, self.cfg.num_pieces - 1), device=self.device)
            for i in range(self.cfg.num_pieces - 1):
                # Transform to world frame
                inner_pts[:, :, i] = quat_rotate(q_odom, self.waypoints[agent][:, 3 * i : 3 * (i + 1)] * self.cfg.p_max[agent]) + p_odom
            inner_pts_all.append(inner_pts)

            # Tail states, transformed to world frame
            p_tail = quat_rotate(q_odom, self.waypoints[agent][:, 3 * (self.cfg.num_pieces - 1) : 3 * (self.cfg.num_pieces + 0)] * self.cfg.p_max[agent]) + p_odom
            v_tail = quat_rotate(q_odom, self.waypoints[agent][:, 3 * (self.cfg.num_pieces + 0) : 3 * (self.cfg.num_pieces + 1)] * self.cfg.v_max[agent])
            a_tail = quat_rotate(q_odom, self.waypoints[agent][:, 3 * (self.cfg.num_pieces + 1) : 3 * (self.cfg.num_pieces + 2)] * self.cfg.a_max[agent])
            tail_pva = torch.stack([p_tail, v_tail, a_tail], dim=2)
            tail_pva_all.append(tail_pva)

            durations = torch.full((self.num_envs, self.cfg.num_pieces), self.cfg.duration, device=self.device)
            durations_all.append(durations)

        head_pva_all = torch.cat(head_pva_all, dim=0)
        inner_pts_all = torch.cat(inner_pts_all, dim=0)
        tail_pva_all = torch.cat(tail_pva_all, dim=0)
        durations_all = torch.cat(durations_all, dim=0)

        MJO = MinJerkOpt(head_pva_all, tail_pva_all, self.cfg.num_pieces)
        start = time.perf_counter()
        MJO.generate(inner_pts_all, durations_all)
        end = time.perf_counter()
        logger.debug(f"Local trajectories generation takes {end - start:.6f}s")

        traj_all = MJO.get_traj()
        self.trajs = {agent: traj_all[i * self.num_envs : (i + 1) * self.num_envs] for i, agent in enumerate(self.possible_agents)}
        self.execution_time = torch.zeros(self.num_envs, device=self.device)
        self.has_prev_traj.fill_(True)

    def _apply_action(self) -> None:
        if self.control_counter % self.cfg.control_decimation == 0:
            start = time.perf_counter()
            for agent in self.possible_agents:
                self.actions[agent] = torch.cat(
                    (
                        self.trajs[agent].get_pos(self.execution_time),
                        self.trajs[agent].get_vel(self.execution_time),
                        self.trajs[agent].get_acc(self.execution_time),
                        self.trajs[agent].get_jer(self.execution_time),
                        torch.zeros_like(self.execution_time).unsqueeze(1),
                        torch.zeros_like(self.execution_time).unsqueeze(1),
                    ),
                    dim=1,
                )

                self.a_desired_total[agent], self.thrust_desired[agent], self.q_desired[agent], self.w_desired[agent], self.m_desired[agent] = self.controllers[
                    agent
                ].get_control(self.robots[agent].data.root_state_w, self.actions[agent])

                self._thrust_desired[agent] = torch.cat((torch.zeros(self.num_envs, 2, device=self.device), self.thrust_desired[agent].unsqueeze(1)), dim=1)

            end = time.perf_counter()
            logger.debug(f"get_control for all drones takes {end - start:.6f}s")

            self.control_counter = 0
        self.control_counter += 1

        for agent in self.possible_agents:
            self.robots[agent].set_external_force_and_torque(self._thrust_desired[agent].unsqueeze(1), self.m_desired[agent].unsqueeze(1), body_ids=self.body_ids[agent])
        self.execution_time += self.physics_dt

    def _get_observations(self) -> dict[str, torch.Tensor]:
        observations = {agent: self.robots[agent].data.root_state_w.clone() for agent in self.possible_agents}
        return observations

    def _get_rewards(self) -> dict[str, torch.Tensor]:
        rewards = {agent: 0 for agent in self.possible_agents}
        return rewards

    def _get_dones(self) -> tuple[dict[str, torch.Tensor], dict[str, torch.Tensor]]:
        died = torch.zeros(self.num_envs, dtype=torch.bool, device=self.device)
        for agent in self.possible_agents:
            z_exceed_bounds = torch.logical_or(self.robots[agent].data.root_link_pos_w[:, 2] < -0.1, self.robots[agent].data.root_link_pos_w[:, 2] > 10.0)
            ang_between_z_body_and_z_world = torch.rad2deg(quat_to_ang_between_z_body_and_z_world(self.robots[agent].data.root_link_quat_w))
            _died = torch.logical_or(z_exceed_bounds, ang_between_z_body_and_z_world > 60.0)
            died = torch.logical_or(died, _died)

        time_out = self.episode_length_buf >= self.max_episode_length - 1

        return {agent: died for agent in self.cfg.possible_agents}, {agent: time_out for agent in self.cfg.possible_agents}

    def _reset_idx(self, env_ids: Sequence[int] | torch.Tensor | None):
        if env_ids is None or len(env_ids) == self.num_envs:
            env_ids = self.robots["drone_0"]._ALL_INDICES

        for agent in self.possible_agents:
            self.robots[agent].reset(env_ids)

        super()._reset_idx(env_ids)
        if len(env_ids) == self.num_envs:
            # Spread out the resets to avoid spikes in training when many environments reset at a similar time
            self.episode_length_buf = torch.randint_like(self.episode_length_buf, high=int(self.max_episode_length))

        self.has_prev_traj[env_ids].fill_(False)

        # Reset robot state
        for agent in self.possible_agents:
            joint_pos = self.robots[agent].data.default_joint_pos[env_ids]
            joint_vel = self.robots[agent].data.default_joint_vel[env_ids]
            default_root_state = self.robots[agent].data.default_root_state[env_ids]
            default_root_state[:, :3] += self.terrain.env_origins[env_ids]
            self.robots[agent].write_root_pose_to_sim(default_root_state[:, :7], env_ids)
            self.robots[agent].write_root_velocity_to_sim(default_root_state[:, 7:], env_ids)
            self.robots[agent].write_joint_state_to_sim(joint_pos, joint_vel, None, env_ids)


gym.register(
    id="FAST-Quadcopter-Swarm-Direct-v0",
    entry_point=QuadcopterSwarmEnv,
    disable_env_checker=True,
    kwargs={"env_cfg_entry_point": QuadcopterSwarmEnvCfg},
)
